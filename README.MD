# ğŸ“Š Credit Risk ETL Pipeline

Pipeline ETL automatizado para processamento de dados de crÃ©dito, orquestrado com Apache Airflow, extraindo dados do AWS S3 e carregando no Snowflake.

---
## ğŸ“‘ Ãndice

- [ğŸ—ï¸ Arquitetura](#ï¸-arquitetura)
- [ğŸš€ Como Executar o Pipeline](#-como-executar-o-pipeline)

## ğŸ—ï¸ Arquitetura

![architecture_image](images/Diagrama_pronto.jpg)

### Detalhes do Pipeline:

**Tasks:**
1. **extract**: Extrai CSVs do S3 â†’ retorna DataFrames
2. **transform**: Aplica transformaÃ§Ãµes â†’ retorna DataFrames limpos
3. **load**: Salva em Parquet â†’ Upload para Stage â†’ COPY INTO tables

**ComunicaÃ§Ã£o entre tasks:** XCom (Cross-Communication)

**Agendamento:**

- **FrequÃªncia:** DiÃ¡ria
- **HorÃ¡rio:** 02:00 UTC
- **Cron:** `0 2 * * *`
- **Catchup:** Desabilitado
- **Retries:** 2 tentativas
- **Retry Delay:** 5 minutos

## ğŸš€ Como Executar o Pipeline

### **PrÃ©-requisitos**

- Docker & Docker Compose V2 instalados
- Credenciais (nesse caso minhas credenciais serÃ£o disponibilizadas para teste)

### **1. Clone o RepositÃ³rio e Mude Para Ele**

```bash
git clone <HTTPS ou SSH aqui>
cd Desafio_neon
```
### **1.2. Configure as variÃ¡ves de ambiente âš ï¸ **IMPORTANTE**
> **ğŸ”´ ATENÃ‡ÃƒO:** Este passo Ã© **obrigatÃ³rio**! O projeto nÃ£o funcionarÃ¡ sem as credenciais configuradas.

- Esse projeto foi construido no desafio de estÃ¡gio de engenharia de dados da NEON, por isso minhas credenciais foram disponibilizadas de forma segura nesse contexto, o que infelizmente nÃ£o posso replicar aqui ğŸ˜”.
- No *.env.example* hÃ¡ um exemplo de como as credenciais deveriam ser preenchidas.

ğŸ”¥ Finalmente execute este comando:
```bash
# Adicione seu UID do sistema
echo "AIRFLOW_UID=$(id -u)" >> .env

```

### **2. Inicie o Airflow**

```bash
# Subir containers
docker compose up -d

# Verificar status
docker compose ps
```
### **4. Acesse a Interface do Airflow**

**Credenciais de acesso:**
| Propriedade | Valor |
|-------------|-------|
| **URL** | http://localhost:8080 |
| **UsuÃ¡rio** | `airflow` |
| **Senha** | `airflow` |

### **5. Execute o Pipeline**

**Na interface do Airflow:**

1. ğŸ” Localize a DAG **`credit_etl_pipeline`**
![example_image1](images/procura_dag.png)

2. âš¡ Ative a DAG (toggle Ã  esquerda)
3. â–¶ï¸ Clique em **Trigger DAG** para executar manualmente
![example_image2](images/acionar.png)

> **Ou** aguarde a execuÃ§Ã£o agendada (diariamente Ã s 2h da manhÃ£).

4. Veja os logs de cada uma das tasks na UI do Airflow, (clique na task â†’ Logs)

### **6. Parar o Pipeline**
```bash
# Parar containers
docker compose down

# Parar e remover volumes (limpa tudo)
docker compose down -v
```
